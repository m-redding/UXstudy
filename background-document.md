# Background Information
## Event Hubs Background information
#### What is an event hub?
Azure Event Hubs is a big data streaming platform and event ingestion service. It can receive and process millions of events per second. Data sent to an event hub can be transformed and stored by using any real-time analytics provider or batching/storage adapters.

#### What are the key components of an event hub?
- Producers: Any entity that sends data to an event hub, publishers can publish events useing HTTPS, AMQP, or Kafka
- Consumer groups: A view (state, position, or offset) of an entire event hub. Consumer groups enable consuming applications to each have a separate view of the event stream. They read the stream independently at their own pace and with their own offsets
- Partitions: An individual consumer only reads a specific subset, or partition, of the message stream
- Throughput units or processing units: Pre-purchased units of capacity that control the throughput capacity of Event Hubs
- Event Receivers: Any entity that reads event data from an event hub. All Event Hubs consumers connect via the AMQP session, the Event Hubs service delivers events through a session as they become available 

#### What are event hubs used for?
Some examples of what event hubs can be used for are the following:
- Anomaly detection (fraud/outliers), for example, looking at credit card transactions and determining when fraudulent purchases are made by seeing if a group of purchases on one card are all in different regions 
- Logging messages generated by application code, the messages can be generated by the web framework or from application code directly 
- Transaction processing - high volume processing of batches of transactions, especially independent transactions that can be processed in parallel. (like financial transactions, experimental data gathered by scientific instruments), this processing needs to maintain integrity in the system and always keeping everything in a consistent state 
- Device telemetry streaming - data is pushed automatically without worrying about polling, this is more efficient, continuous, scalable and allows better access to real-time data, it is also good for automation, optimization, preventing troubleshooting, and load balancing

#### What are producers?
A producer is a client responsible for publishing or sending event data to a specific Event Hub. Currently this is done by grouping sets of event data together in batches. Depending on the options specified when sending, event data may be automatically routed to an available partition or sent to a specifically requested partition.

## Introducing the Streaming Producer
Publishing events using the **producer client**, as described briefly above, is optimized for high and consistent throughput scenarios, allowing applications to collect a set of events as a batch and publish in a single operation.  In order to maximize flexibility, developers are expected to build and manage batches according to the needs of their application, allowing them to prioritize trade-offs between ensuring batch density, enforcing strict ordering of events, and publishing on a consistent and predictable schedule.

The primary goal of the **streaming producer** is to provide developers with the ability to queue individual events for publishing without the need to explicitly manage batch construction, population, or service operations.  Events are collected as they are queued, organized into batches, and published by the streaming producer as batches become full or a certain amount of time has elapsed.  When queuing events, developers may request automatic routing to a partition or explicitly control the partition in the same manner supported by the `ProducerClient`, with the streaming producer managing the details of grouping events into the appropriate batches for publication.

#### What are the advantages of the streaming producer?

When applications need to process low frequency or sparse event streams, the current approach for publishing a single event can introduce inefficiencies that could negatively impact throughput. In order to avoid this, developers need to include non-trivial overhead to their code to manage decisions around caching batches-in-progress, routing events to the correct batch, and publishing partial batches when events aren't being published frequently . With the streaming producer, applications can queue events into the producer as they are needed and the producer will take care of efficiently managing batches and publishing. 

A method to publish a single event was available in legacy versions of the Event Hubs client library. However, this method used a na√Øve approach of  simply publishing a batch containing one event, rendering it highly inefficient.  Developers often assumed that there was an intelligence behind the method, expecting it to leverage efficient batching, leading to overuse and poor application performance. The streaming producer returns the ability to publish a single event, but with a more efficient implementation that helps to ensure throughput and reduce resource use.